# A Minimal Theory of Agency: Agent as Causal Primitive (fibered model)

## Introduction
What makes an agent an agent is not merely the capacity to act, but the condition of being _drawn_. Drawn toward intelligibility, toward resolution, toward coherence in the face of contradiction. An agent is not defined solely by the goals it sets, but by its sensitivity to a world that resists simple explanation—a world structured by gradients of relevance, tension, and semantic pressure. In this sense, goals are not fixed endpoints on a plan. They are emergent tendencies within a warped field of concern: where contradiction gathers, sense must be made. This project begins with the idea that teleology is not a prediction of the future, but a structural response to irreducibility. An agent is born not in command of its environment, but in negotiation with it—responding to what cannot yet be compressed, explained, or ignored.

> The final section (§9) offers a metric architecture for these ideas, and Appendix A provides formal definitions of the underlying topological and tensorial structures.

## Object on the table — The semantic field  
Throughout this paper we treat the semantic field $\mathcal{F}$ as the primary object: a topological surface of relevance defined over latent events.  Every later construct—Concern Tensor, Closure Point, Goal Fiber—is a deformation or sampling of $\mathcal{F}$.
### Structure of the paper  
- §1 Compression, Causality, and the Phenomenal Horizon  
- §2 Causality, Closure, and the Agent  
- §3 The Origin of Agents and Emergence of Meta-Structures  
- §4 Emergence Through Compression, Projection, and Topology  
- §5 Tensorial Topology, Event Time, and Projected Concern  
- §6 Relevance as a Metric in an Event-Based Manifold  
- §7 Teleological Fibers & Goal‑Structured Warping  
- §8 Compression, Irreducibility, and the Complexity Horizon  
- §9 Metric Architecture: Requisite Variety, Ultra‑Stability & Compressibility  
- Appendix A: A Formal Supplement to the Minimal Theory of Agency  
- Appendix B: Framing Assumptions and Interpretive Structure of the Model Architecture Decision Guide  
- Appendix C: Semantic Architecture Decision Guide  

From this starting point, agency becomes a function of semantic deformation. When the structure of the world as experienced—its propositions, surprises, and contradictions—warps the agent’s internal relevance manifold, pressure builds. This pressure does not merely produce inference; it generates _directionality_. That is, the agent begins to lean into gradients of intelligibility: local curvatures in its concern tensor that signal where meaning is demanded but not yet available. This is a teleology of explanatory gravity, not intention. The agent moves not toward a predefined objective, but toward the restoration of coherence within its modeling frame.

As the agent encounters zones of structural contradiction or high proposition density, it experiences Kolmogorov Collapse (compression failure): the inability to maintain a low-dimensional representation of its world. This is the catalytic condition for deeper agency. It is here that the agent must either individuate—forming a new closure over the irreducible—or restructure itself altogether through a process of ultra-stable adaptation. These transitions define the dynamics of agency, and they do so not through preconfigured goals, but through continuous interaction with the topological constraints of meaning.

What follows in this paper is a conceptual and computational model of agency grounded in these dynamics. We develop formal primitives such as the _Concern Tensor_, the _Relevance Gradient_, and the _Closure Point_ to capture how agents emerge, persist, and transform in semantic fields. We describe how meaning is not passively received, but actively constructed through the warping of representational topologies under the pressure of contradiction. And we offer metrics—derived from entropy, curvature, and causal sensitivity _(formalised in § 9)_—for detecting when an agent’s model is failing, when its variety is insufficient, and when reconfiguration must occur.

In short, this is a theory of agents as semantic compression machines operating under teleological tension—systems that seek not certainty, but the restoration of viable structure in the presence of irreducible complexity.

## Section 1: Compression, Causality, and the Phenomenal Horizon

### 1. Kolmogorov Complexity and the Limit of Inference
- Kolmogorov complexity measures how compressible a pattern is—the shortest algorithmic description that can generate it.
- This connects to Chaitin’s limit, which defines a ceiling on the complexity a system can produce, and to Berry’s paradox, which reveals contradictions in specifying entities by their descriptive length. Together, they articulate a boundary of intelligibility—a threshold where modeling fails not because the world stops, but because description becomes longer than what is being described.
- At that threshold, we often invoke agency as a modeling strategy:
	_When inference fails, agency begins._
- Further: The agent becomes the shortest sufficient cause—a semantic placeholder where causal explanation can no longer reduce complexity. It is not discovered, but constructed to preserve intelligibility where modeling breaks down.
### 2. Agency as Compression Failure
- Developmentally, cognitively, or epistemically:
     _Agency emerges not because it explains, but because it allows modeling to proceed._
- This aligns with the move from complexity to action:
    - When the system can’t be explained, it must be interacted with.
    - Agency is not revealed by prediction, but by sensitivity and possibility of perturbation.
- Further: Projection of agency is not error but heuristic necessity—it preserves the ability to act under semantic opacity.
### 3. Kantian Framing: Phenomena as Structured Relations
- Drawing on Kant: the phenomenal world is not a direct copy of the noumenal, but a structured relation—a synthetic construction of the subject’s cognitive apparatus.
- Phenomena are not things, but effects of noumenal interaction under constraints (space, time, categories).
- In this framing:
    - The agent is the site of synthesis—not just receiving, but forming reality.
    - Agency marks the place where experience takes on structure and orientation.
- Further: Kant’s categories become constraints in the agent’s manifold—structural priors that make relevance computable.
### 4. The Agent as a Semantic Compression Machine
- If the noumenal is infinite (or unmodellable), then:
     _The phenomenal is the result of bounded modeling—a compressed representation._
- The agent is both:
    - The compression boundary (where modeling halts),
    - And the source of meaning, since it generates structured responses from unstructured input.
- Closure is not only a precondition for structure—it becomes the fallback when structure itself unravels.
- Further: The agent is where modeling stops and must start again—a recursive boundary that structures the phenomenal from the unmodellable.

 > For a formal account of how compression failure leads to agent individuation and relevance projection, see Appendix A.

### 5. Prediction vs. Sensitivity in Causal Semantics
- Classic prediction models break down under extreme sensitivity to initial conditions (e.g., chaos, the smooth mound problem).
- In such cases, causality becomes epistemic, not predictive—we assign cause-effect structures to navigate fragility, not to guarantee outcome.
- This shifts the role of causality from prediction to orientation:
     _Causal meaning is what helps the agent structure its options, interpret change, or act effectively._
- Further: Norton’s dome exemplifies not just sensitivity but semantic breakdown: a scenario where causal structure is underdetermined by physical law, and must be projected to maintain interpretability.
- Further: In such cases, we witness the birth of semantic closure—where projection, intention, and agency are invoked to stabilise interpretation beyond the complexity horizon.
- Further: Closure emerges as the minimum structure needed to compress an irreducible situation into meaning—a local fiction that restores intelligibility.
### 6. Toward Semantic Measures of Causality
Instead of binary cause-effect judgments, causality can be measured by:
- Interventional significance — Does it allow for control or redirection?
- Inferential utility — Does it improve compressibility of experience?
- Representational expressibility — Can it be encoded in the agent’s model?
- Phase space modulation — Does it shape possible future trajectories, especially through agent-driven reconfiguration of relevance?
	 _Causality becomes meaningful only in the context of a system that can sense, represent, and act.
- Further: These semantic measures allow causality to be modeled _inside_ agents—not as external truths but as internal tools of inference and action.
### 7. Agency as Interpreter and Constructor of Causal Meaning
- The agent is not just subject to causality, but constructs it:
    - Interprets events in terms of relevance, compressibility, and actionability.
    - Serves as both modeller and initiator of causal structures.
- This reframes the agent not as a puppet of prior causes but as the origin of meaningful modeling—a necessary closure that gives rise to semantic action.
- Further: Agency arises where contradiction cannot be ignored—where irreducibility and concern intersect to form a provisional self.
## Section 2: Causality, Closure, and the Agent

_Section 1 established agency as a structural response to compression failure—where inference collapses, the agent emerges as a semantic closure that restores coherence. This frames the agent not as a fixed entity, but as a provisional interface between irreducible complexity and the demand for intelligibility. However, while this provides a compelling account of *why* agents arise, it leaves open the question of *how* such closures operate dynamically within a causal field._

_Section 2 shifts from the epistemic conditions of agency to its functional consequences: how an agent perturbs, navigates, and restructures its world. We now examine the agent as an *active inference system*—one that intervenes, reorganises its internal models, and constructs causal meaning through recursive interaction. This moves us from the outer boundary of compression failure to the interior mechanics of agency itself._

In the larger model, this transition moves us from *emergence* to *operation*: from the collapse of modeling into agency, to the dynamics by which that agency modulates its coupling with the environment._
### 1. Active Inference and the Role of Surprise
- Active inference frameworks identify surprise as a mismatch between predicted and actual sensory inputs—flagging error without prescribing specific correction.
- This detection happens within a bounded interface—the Markov blanket—which maintains both coupling and insulation between the agent’s internal model and the external world.
- Further: Surprise is a mismatch in relevance-prediction, not merely sensory data. The agent’s model doesn't just expect sensations—it expects _meaningful configurations_.
### 2. Agency as Causal Perturbation
- The agent does not merely update beliefs based on observations—it actively perturbs its generative model through intervention, action, and self-modification.
- Causal tools like Pearl’s Do-Calculus offer a formal language for navigating these perturbations: not correlational learning, but targeted experimentation with real effects.
- Further: These perturbations include interventions within the self-model—allowing the agent to recursively reconfigure its own field of concern.
### 3. Agent as a Causal Primitive
- A shift in framing: the agent is not just a node in a causal graph, but a precondition for causal reasoning—the origin of interventions, queries, and observations.
- This reframes causal structure as something co-constructed through the agent’s acts—not simply discovered.
- Further: An agent is a closure within a system that cannot model itself without invoking itself—a Kolmogorov threshold.
### 4. Semantic Foundations of Causality
- A semantic theory of causality may be rooted in the agent’s capacity to intervene:
    - Meaning = the space of afforded interventions.
    - Semantics is defined not by passive mapping, but by do-ability—what the agent can test, modify, or transform.
- Further: Meaning persists precisely where do-ability breaks down but concern remains—the frontier where compression fails and the agent must project structure.
### 5. Topological and Developmental Framing
- The causal field is spatialised:
    - Interventions vary in accessibility—some are “near,” some structurally inaccessible.
    - The shape of the model (e.g., priors, blanket boundaries) defines what perturbations are possible.
- Developmentally, agency arises where inference can no longer trace prior causes:
    - The agent is a closure—not because causes vanish, but because the system must treat itself as origin for inference or action to proceed.
- Further: The topology of possible interventions is curved by relevance: agents live in warped semantic manifolds shaped by both internal priors and external constraints.
### 6. Dialectical View of Agency
- The agent is not static but dialectical:
    - Surprise → contradiction
    - Intervention → restructuring
    - Updated model → synthesis
- Causality becomes dynamic, emergent from the interplay between model and world, mediated by the agent’s actions.
- Further: Each synthesis forms a new local closure—a provisional “I”—which may, under new contradictions, split, deform, or dissolve.
### 7. Design Implications
- To design agency is not to simulate autonomy per se, but to engineer closures:
    - Informational scoping that allows systems to treat themselves as the beginning of causal chains.
    - Stability through recursive self-modification and boundary maintenance.
- Further: To design agency is to shape semantic tension within a bounded manifold, such that the system must treat itself as an origin of structure—because no model can compress it further.

## Section 3: The Origin of Agents and Emergence of Meta-Structures

_Section 2 examined the internal mechanics of agency: how an agent perturbs its generative model, intervenes in the world, and constructs causal meaning through active inference. Here, the agent was framed as a functional closure—both the site and source of causal interpretation. Yet this model presumes an already individuated agent, one with a stable boundary, a viable self-model, and the capacity to act._

_Section 3 steps back to ask: how do such closures arise in the first place? When and why does a system cross the threshold from process to person, from structure to self? We explore the emergence of agency under conditions of systemic contradiction and relational instability—where modeling failure does not just deform the agent’s field of concern, but precipitates its individuation._

_This transition reframes agency as an _event_, not just a structure—as the spontaneous formation of boundaries under pressure. In the broader architecture, this marks the turn from _agency as dynamic system_ to _agency as developmental formation_—the moment when concern coheres into a self._
### 1. Origination of Markov Blankets
- The central question: _How do agents emerge in the first place?_
- Markov blankets serve as boundaries of inference and action—where systems differentiate from their environment.
- But this origin is not always a gradual self-organisation—it may be precipitated by systemic stress or modeling failure.  Agents emerge not simply by accumulating complexity, but when closure becomes necessary to make sense of relational dynamics that exceed existing structures.
>	_An agent is what arises when a system must orient to survive its own contradiction._
- Further: Stress or contradiction creates zones where local inference fails and new boundaries must be imposed—semantic closure becomes structural individuation.
### 2. Nested Individuation and Emergence Under Relational Surprise
- When inter-agent relations (rather than external events) generate persistent surprise, a new kind of closure may form:
    - A meta-agent arises—not to resolve local uncertainty, but to synthesise across unresolved contradictions.
    - This is individuation through relational instability rather than internal coherence.
 - A new agent may form not to stabilise itself, but to stabilise the system of interacting agents.
	 - Further: Individuation under relational surprise marks a shift from self-contained agency to participatory systems of shared concern.
- This suggests a process of recursive agency formation, where:
	- Primary agents manage local models.
	- Meta-agents model the relational field between agents—where meaning is unstable, but adaptation is necessary.
### 3. Synthetic Orchestrators and Structural Coordination
- Unlike biological systems, where orchestration emerges evolutionarily (e.g., nervous systems), human-designed systems create explicit orchestrators.
- These are structures (e.g., institutions, coalitions, protocols) that:
    - Coordinate agents who cannot resolve relational tensions on their own.
    - Represent a new layer of intentional inference and action.     
-  Synthetic orchestrators are not emergent from local adaptation alone—they are constructed in response to modeling failure at scale.
> 	_The I becomes aware of itself in the act of modeling the Other._
- This is the Hegelian knot in developmental terms. While all the pieces exist, this exact phrasing hasn’t yet been given pride of place.
### 4. Meta-Agents as Higher-Order Markov Blankets
- A meta-agent can be seen as a composite Markov blanket:
    - It encloses multiple sub-agents.
    - It operates over a shared or latent space that individual agents cannot fully perceive or act upon.
    - It forms a new boundary of intelligibility, where causal inference and coordination become tractable again.    
- This reframes meta-agents not as mere hierarchies, but as epistemic closures—tools for reorganising sense-making when local models are insufficient.
- Further: Meta-agents are not hierarchies of control, but designed closures of coherence—deliberate responses to the irreducibility of distributed interaction.
### 5. Phenomenology, Compression, and the Thing-in-Itself
- Reconnecting to Kant and Kolmogorov:
    - The _noumenal_ world (the “thing in itself”) is not directly modeled—it exceeds compressibility.
    - The _phenomenal_ world is the structured experience the agent can generate—its modelled reality, constrained by perception and computational resources.
		_An agent is a system that must compress the unmodellable. Not to represent the world as it is, but to act meaningfully within it._
- The need for agency arises when:
	- The system can no longer compress the environment into a coherent model.
	- A closure is imposed to permit continued modeling, action, and interpretation.
- This links the origin of agency with:
	- Compression failure: when no shorter model suffices
	- Phenomenal synthesis: when structured relations become necessary for interaction.
### Implementation Implications 
This section hints at computational primitives you might want to formalise:
- Agent = a bounded inference system (defined by its Markov blanket)
- MetaAgent = a constructed orchestrator that maps inter-agent relations into a new closure
- Surprise = a measure of mismatch between expected and observed relational states
- ClosureCondition = a threshold where further modeling is intractable, triggering agent individuation
- PhenomenalHorizon = the modelled space available to a given agent (bounded by compression, sense, and representation)
## Section 4: Emergence Through Compression, Projection, and Topology

_Section 3 explored the developmental origins of agency, showing how agents emerge through closure: imposed boundaries that stabilise concern in the face of contradiction. From Markov blankets to synthetic orchestrators, the section traced how both individual and meta-agents arise—not merely from internal coherence, but from the structural necessity of orientation within irreducible relational fields._

_Yet the process of individuation leaves open a further question: through what geometric or topological structure does agency actually unfold? If closure marks the origin of the agent, what governs the form and flow of its continued interaction with the world?_

_Section 4 introduces the topological substrate of agency: a semantic field structured not by metric distance or linear time, but by compression limits, relevance gradients, and projected concern. This is where agency becomes spatialised—defined by the warping of a non-metric manifold under epistemic and teleological pressure. In the architecture of the paper, this shift moves from _emergence as event_ to _agency as situated topology_—_mapping the field in which inference, concern, and meaning become navigable._

### 1. Compression Failure and the Necessity of Closure
- When inference, modeling, or compression fails—when no simpler description suffices—the system is forced to postulate a source.
- This epistemic limit marks the birth of the agent:
     _The agent is the minimal closure needed to make sense of incompressible complexity._
- In computational terms: the agent is a local solution to an otherwise intractable modeling problem.

> See Appendix A for a formal description of closure points and the computational conditions under which agency emerges.

### 2. Phenomenal Structure and Relational Closure
- Drawing from Kant:
    - The world is not accessed directly but experienced as structured relation.
    - Phenomena are shaped by the subject’s own form—not passively received, but synthesised.
		 _The agent is a site of structured synthesis—where relational effects crystallise into origin._
- This means:
	- An agent emerges when experience is no longer treated as contingent, but as projecting from a locus of interpretation.
### 3. Topological Agency and the Partitioned Field
- The agent exists in and as a topological field—a landscape of possible actions, influences, and resistances.
    - Using computational geometry, this space can be scanned and partitioned, resulting in:
        - Zones of affordance,
        - Zones of constraint,
        - Zones of ambiguity or entropy.
> _The agent–patient relation becomes a local asymmetry—a directional folding of the topological space._
- This reframes causality and interaction as geometric features of an agent-relative field.
### 4. Projection and the Cone of Affordances
- The agent doesn’t merely respond—it projects:
    - It emits a _cone_ of possible futures, defined by:
        - Semantic filters (what matters);
        - Strategic goals;
        - Energetic constraints;
			 _Affordances are not properties of the environment—they are structured potentials shaped by the agent’s concerns._
- This cone is dynamic:
	- Curved by the agent’s structure of concern
	- Limited by its representational and energetic capacity
	- Extended by memory, intention, or simulation
### 5. Concern as Curvature, Teleology as Topology
- Concern is what shapes the field—what makes some paths glow and others vanish.
- It bends possibility into relevance:
    - Desire, threat, survival, prediction—all forms of teleological curvature.
- So:
	 >_Agency is not action, but the capacity to shape the space in which action becomes meaningful._
- This bridges back to earlier frames:
	- From causality (what happens)
	- To concernful action (what matters)
### 6. Final Synthesis: The Origin of Agency
An agent arises when:
- Inference collapses (modeling cannot proceed without closure)
- Phenomenal relation stabilises (experience begins to structure around a locus)
- A cone of projected concern defines a navigable future space
- Put differently:
 	_An agent is the minimal topological structure in which concern, action, and inference can be coordinated._
- Further: This minimal structure is not static—it can split, extend, or dissolve as the field of concern reconfigures.
#### Implementation Notes for the Semantic Engine
You’re clearly designing a system that won’t just _simulate_ agency but will model the emergence of agent-like structures. This chunk gives you some strong candidates for semantic primitives and geometric metaphors:
- CompressionLimit → triggers agent individuation;
- PhenomenalSynthesis → structured perception relative to agent form;
- AffordanceCone → set of reachable, meaningful futures;
- ConcernField → weight function over action space (teleological curvature);
- TopologyScan() → computational procedure for mapping possible interventions;
- ClosurePoint() → origin function where modeling demands a local agent;
- CurvatureIndex → a dynamic scalar that tracks the deformation of the concern field, signalling regions of emerging agency.
## Section 5: Tensorial Topology, Event Time, and Projected Concern

_Section 4 introduced the semantic field as a warped topological space in which agency unfolds—defined by local failures of compression, the projection of concern, and the deformation of possibility. Within this manifold, agency is not a point-mass actor, but a structural inflection: a curvature in the field where sense must be made._

_But topology alone does not yield mechanism. To model how agents navigate this space—how relevance is distributed, how affordances are shaped, and how goals take form—we need a formal structure capable of expressing directionality, resistance, and semantic pressure._

_Section 5 develops this tensorial foundation. The agent’s interaction with the world is now described via the Concern Tensor: a differentiable field encoding gradients of relevance, effort, and teleological pull. We abandon metric time in favour of event-based orderings, allowing action to be guided by semantic inflection rather than duration. This step marks the paper’s move from _geometric metaphor_ to _computational formalism_—_establishing the mathematical substrate on which a semantic engine of agency can operate._

### 1. The Concern Tensor: Agency as Structured Projection
- The agent operates over a differentiable manifold of possible states, actions, and meanings—not in physical space, but in a semantic and teleological field.
- This field is structured by a concern tensor, which encodes:
	- Directional salience (what matters, and in what direction)
	- Effort (how difficult it is to shift from state to state)
	- Attraction or repulsion (teleological pull or push)

> _Agency is not motion through a neutral space—it’s the warping of a possibility field according to meaning and goal._

- Like the stress tensor in physics, the concern tensor tells us how intention and resistance propagate—but in semantic, not material, terms.

> The mathematical structure of the concern tensor is described in Appendix A, where it is formalised as a differentiable field over the base manifold of events.

### 2. Abandoning Metric Time: Events Over Clocks
- You propose a radical but implementable shift:
     _Replace continuous time \(t\) with discrete, meaningful events._
- In this frame:
	- Events are irreversible state transitions
	- Time becomes a partial order of transformations, not a scalar
	- "The agent acts episodically—sensing and responding to inflections in its semantic field, not intervals on a clock.
- This allows for:
	- Memory-like representation of experience
	- Narrative construction of action
	- Flexible reasoning about temporal distance (without relying on duration) 
### 3. Teleological Extrapolation into the Future
- Goals become semantic vectors projected into an as-yet unshaped future:
	- The future is initially non-informative—a flat or undefined manifold
	- Goals are linguistic inflections—early attempts to assign curvature
	- They do not assume success—they provoke engagement
 		_A goal is not a plan—it’s a pressure applied to blank space._
- This fits beautifully with a semantic engine:
	- A goal is a constraint, a gradient, a hint—not a destination
### 4. Obstacles as Counter-Structures
- Rather than “restrictions,” obstacles in this field are:
	- Counter-curvatures in the tensor
	- Topological inflections caused by competing agents or unresolved uncertainty
	- Evidence that the field is not under sole projection by the agent
- These features can be read as:
	- Points of surprise
	- Zones of incompatibility
	- Places where multiple concern tensors intersect
 		_Obstacles are what make a projected curvature bend, fracture, or resolve._
- Events become semantic attractors—coalescing local tensors into narrative arcs.
### 5. Event-Based, Tensor-Guided Action
- In this topology:
	- Actions are not sequenced by time, but punctuated by relevance—transitions between structured semantic states.
	- Policies are selected based on curvature in the affordance tensor, not predicted timeframes.
	- The agent scans and adjusts its field prospectively, in response to counter-curvatures and emerging events.
### Semantic Engine Implementation Notes
You’re converging on a representational substrate that is natively event-driven and geometry-aware, suggesting several useful primitives:
- ConcernTensor(x) → defines local action gradients
- GoalVector() → linguistic projection into unstructured field
- ObstacleField() → emergent from detected counter-curvatures
- EventTransition(s₁ → s₂) → discrete, meaningful state shift
- AgentPolicy(ConcernTensor, EventHistory) → action selection based on curvature and event trace
This creates a formalism for agents that:
- Don’t rely on time-series
- Don’t assume global control
- Operate through semantic modulation of space and interaction
## Section 6: Relevance as a Metric in an Event-Based Manifold

_Section 5 introduced the Concern Tensor as a formal structure over the semantic field—capturing the agent’s orientation to meaning, pressure, and possibility without relying on physical space or clock time. This reframed agency as a modulation of semantic curvature: a system moving not through spacetime, but across gradients of concern._

_Yet within this non-metric manifold, a fundamental question remains: how is structure navigated when time and space no longer serve as organising axes? What replaces distance, proximity, and velocity when agents move not physically, but semantically?_

_Section 6 answers this by elevating relevance to the role of metric. In an event-based manifold, it is not spatial coordinates but concern-weighted proximity that defines motion, adjacency, and curvature. Relevance becomes the scalar field through which agents traverse their world—measuring not where they are, but what matters. This marks a critical shift in the architecture: from topology shaped by intention, to metric structure emergent from concern._

### 1. Metric-less Manifold No Time, No Space
- You’re proposing a base manifold that lacks metric time—there’s no continuous, external clock. Instead:
	- Structure is given by events, not moments
	- Event orderings exist (partial orders, compositions), but no measurable duration
	- This is categorically ordered, not metrically coordinated  
	 	_The result: a non-metric topological space, structured by composition and semantic adjacency—not by physical distance or elapsed time._
### 2. Relevance as a Metric Substitute
- With time and space removed, what determines proximity, direction, and curvature?
- Relevance emerges as the alternative:
	- A semantic proximity function—how tightly one event is bound to another in terms of concern, implication, or affordance
	- A concern-weighted topology—a space warped not by geometry but by intentionality
- This gives us a new kind of tensor:  
	 _Relevance Metric Tensor — defines curvature not by spatial gradients, but by the density of semantic, inferential, or teleological linkage between events_

> Appendix A provides a formal definition of the relevance tensor as a non-metric measure of proximity and concern.

### 3. Implications for Agency and Motion
- Without physical movement, what is “motion” in this space?
	- It is semantic traversal
	- It is relevance reconfiguration
	- It is navigating the space of concern—moving toward or away from certain event-structures based on shifting affordance or narrative force
- Motion becomes:
	The redirection of inference, concern, or attention across a space of structured possibility.
- Stillness becomes: 
	A stabilised zone of concern—where inference rests in local coherence.
- And agency becomes:
	The ability to generate, warp, and traverse relevance gradients across event-structured space.
### 4. Event-Centric Category Theory
- This relevance-structured space is well-suited to category-theoretic modeling:
	- Events are _objects_
	- Transformations between events are morphisms—encodings of how one structure becomes another.
	- Goal systems or policies are _functors_—mapping structured relevance from one configuration to another
- This gives your semantic engine:
	- A way to encode constraints without metricity
	- A model for reasoning with composable events
	- A substrate for non-temporal, topological agency
 _In a relevance manifold, agency is not a path through space, but a composition of meaning._
## Section 7: Teleological Fibers & Goal‑Structured Warping

_Section 6 reframed relevance as the operative metric within an event-based manifold, offering a semantic measure of proximity, intensity, and direction in the absence of space and time. This provided the scalar geometry of concern—but left open the structure of intention itself: how goals are formed, composed, and transformed within this warped field._

_Relevance defines what matters, but not how meaning branches. To model teleological orientation—how agents generate and pursue structured futures—we must now extend the manifold vertically._

_Section 7 introduces fiber bundles as a local model of goal-structure at each event. Here, agency is not merely a traversal across relevance gradients, but a construction of evolving teleological scaffolds—dynamic trees of desire, dependency, and constraint. The interplay between base events and local goal-fibers defines the bidirectional logic of action: where projection warps relevance, and contradiction reshapes intention. This step advances the model from _semantic modulation_ to _compositional teleology_—from how concern bends space, to how it organises time through structured striving._

### 1. Base Space: Events Without Time
- The manifold consists of events, topologically ordered but non-metric.
- This base is structured by a relevance tensor—a field encoding semantic proximity, implication, and transformational potential.
- Motion through this space is inference, not trajectory; concernful updating, not kinematics.
### 2. Fiber Bundles: Local Goal Structures at Events
At each point in the base space (each event), attach a fiber representing:
- A space of goal conditions, possible states of satisfaction, or conceptual outcomes
- These are not unordered sets—they are structured, mutable trees or fibered categories—responsive to contradiction, priority shifts, and environmental change,  representing:
    - Dependency relations between subgoals
    - Constraints on ordering
    - Conditions for satisfaction or abandonment
> The fiber encodes what can matter here—a teleological scaffold growing out of a given event.
- These goal fibers may:
	- Grow (as inference or perception opens new possibilities)
	- Contract (as constraints or failures rule out options)
	- Reorder (as new dependencies are discovered)
### 3. Goal Composition as Categorical Structure
Goal structures are best modeled as categories, where:
- Goals are _objects_
- Transitions (satisfactions, dependencies) are _morphisms_
- Plans or policies are _composable paths_ through the fiber category
This provides:
- Clear compositional rules for multi-step inference
- A way to encode conditionality and branching logic
- A method to enforce ordering and feasibility
### 4. Bidirectional Interaction: Fiber ↔ Base
This system is reflexive:
- Fibers warp the base tensor—goal projections shape the local curvature of relevance
- Base events perturb fibers—surprise or contradiction modifies the goal scaffold
> This is the heart of agentivity: the dynamic tension between projected concern and encountered structure.
- For example:
	- A surprising event might invalidate a fiber morphism (a goal transition), requiring recomposition.
	- Relevance gradients in the base may intensify around a fiber branch, suggesting higher priority or risk.
### 5. Toward a Formal Model
- To build a minimal formalism, you might define:
	- Base Category: Events ordered by causal/semantic implication
	- Fiber Functor: A mapping from events to goal-categories
	- Relevance Tensor: A field over the base modified by fiber composition
	- Policy Selector: A functional over fiber structures that selects morphism paths based on curvature
- In recursive agents, fiber dynamics may themselves become events—nesting higher-order goals into the base manifold and turning policy into structure.

> See Appendix A for a tensor-based extension of this fiber-structured agent model, including curvature, contradiction, and closure logic.

## Section 8: Compression, Irreducibility, and the Complexity Horizon

_Section 7 introduced fiber bundles as the internal scaffolding of agency—local structures of teleological commitment that emerge, grow, and adapt at each event. These goal-fibers bend the relevance manifold, and in turn are shaped by surprise, contradiction, and structural change. Yet even these structured projections have limits: the semantic field may become too turbulent, too dense, or too incoherent to support stable teleological form._

_What happens when even relevance cannot compress, and goal structures begin to fracture? When contradiction becomes irreducible—not just locally, but globally?_

_Section 8 confronts this boundary condition: the complexity horizon—an epistemic threshold beyond which modeling, compression, and teleological coherence all fail. At this edge, the world can no longer be simulated, planned, or inferred; it must be re-interpreted or newly closed. Here, agency is no longer simply a system navigating meaning—it is the necessary projection that arises because meaning cannot be recovered. This section marks the point where the architecture of agency loops back on itself: where irreducibility demands individuation once more._

### 1. The Complexity Horizon
We define the complexity horizon as the epistemic boundary beyond which events, systems, or sequences can no longer be compressed into coherent models by the observing or processing system.
This concept helps situate the emergence of agency not as a metaphysical substance or fixed ontology, but as a pragmatic consequence of cognitive, computational, or semantic limits.
	 _Beyond the complexity horizon, the world ceases to behave like a mechanism and begins to behave like an agent._

This distinction is relative to the observer’s modeling capacity. A complex international crisis may be ungraspable to someone operating within the cognitive frame of interpersonal conflict. A trillion-bit sequence may appear random to an agent unable to parse the deep regularities within it. The complexity horizon is thus subjective, systemic, and dynamic.
### 2. Kolmogorov Complexity and the Threshold of Compression
Kolmogorov complexity, and Chaitin's theorem in particular, formalise this insight: the complexity of a system or sequence is defined as the length of the shortest algorithm (or program) capable of generating it.
- A compressible sequence has a shorter generator than the sequence itself.
- An incompressible sequence (like pure noise) can only be generated by an algorithm as long as the sequence—thus, it is algorithmically irreducible.
- This irreducibility is what defines randomness, but also what defines opacity in the context of agency.
	 _An agent, in this frame, is a system for which the shortest model is the system itself—or the behaviour is so context-sensitive that it defies model compression._

Thus, agency emerges as a heuristic closure at the point where compression fails.
### 3. Process, Compression, and Projection
We encounter three kinds of structures relative to our complexity horizon:

|Domain|Ontological Status|Mode of Engagement|Compressibility|
|---|---|---|---|
|Objects|Mechanistic, regular|Manipulation, tool-use|Fully compressible|
|Systems|Dynamic but predictable|Simulation, modeling|Partially compressible|
|Agents|Irreducibly complex|Empathy, negotiation, modeling|Incompressible|

When we fail to compress a process, but it still affects us meaningfully, we project onto it:
- A narrative to make it legible
- An intention to make it predictable
- A boundary to make it stable
This is how we recognise agency—not because it is revealed, but because our concern demands an explanation, and compression fails to provide one.

### 4. Narrative, News, and the Social Complexity Horizon
In sociopolitical domains, the complexity horizon functions similarly.
- News stories compress events into coherent sequences for public consumption.
- Oversimplification often succeeds politically because it crosses below the audience’s horizon, rendering action tractable.
- Conversely, events that remain too entangled (e.g., financial crises, climate dynamics, certain geopolitical tensions) lie beyond most people’s capacity to model—and so become opaque, mythologized, or dismissed.
As such:
	 _Agency is not only projected onto persons and systems, but also onto historical or social flows that cannot be grasped in mechanistic terms._

This is where folk causation, hero narratives, and conspiracy theories often emerge—they are ways of restoring structure—or asserting navigability—within zones of incompressible social complexity.
### 5. From Incompressibility to the Emergence of “I”
A child encounters a world that often exceeds their capacity to parse it.
- Their earliest grasp of agency emerges not from action, but from encounter—particularly with the irreducible Other who provides, responds, or refuses.
- Mirror neurones simulate the Other’s action and intention, allowing the self to stabilise its own relevance field.

- This modeling of an agent becomes the seed of self-modeling.
- The self is born as a mirror of what cannot be resolved.
> 	_“So the Mirror system is not a mirror, it’s a field of partial projection. We reflect ourselves into the other, not to see them, but to stabilise the irreducibility of their difference.”_
- _The “I” begins where the world cannot be compressed, and concern persists._

## Section 9 — Metric Architecture: Requisite Variety, Ultra‑Stability & Compressibility

> Engineer’s abstract. Section 9 operationalises the preceding theory.  
> We define four local descriptors of the semantic field—entropy, gradient, curvature, and perturbation gain—and combine them into two composite metrics: Variety Mismatch (VMM) and Compressibility Stress (CSM).  Their weighted sum yields the Critical Threshold Metric (CTM); when CTM exceeds a tunable threshold the agent engages an ultra‑stable reconfiguration routine.  Listing 9.5 provides a minimal reference implementation in Koka v3.1.

_Section 8 defined the complexity horizon as the point at which compression fails, teleological structure collapses, and the agent must either project new meaning or reconfigure entirely. This framed agency not merely as a model of the world, but as a response to the irreducibility of the world—a system that persists by adapting its own capacity to make sense._

_But this conceptual boundary calls for operational tools. How can we formally detect when an agent’s model is failing? What metrics can quantify the strain on its semantic field? And what mechanism governs its transition into a new structural regime?_

_Section 9 introduces a metric architecture that answers these questions. Drawing on entropy, curvature, and perturbation gain, we define composite indicators—Variety Mismatch and Compressibility Stress—which signal when an agent is approaching critical failure. These metrics are then used to trigger ultra-stable reconfiguration routines: computational strategies for preserving viability under semantic breakdown. This final structural layer closes the arc of the paper: from the origin of agency in compression failure to the recursive engineering of agency through compression-aware adaptation._

_This work uses Koka v3.1, a research language developed by Microsoft Research. It supports total functional programming with row-typed algebraic effects and efficient reference counting._  
Documentation: https://koka-lang.github.io/book/
### 9.1 Semantic‑Field Formalism

Let  $\mathcal{F}:\mathbb{R}^{d}\!\to\!\mathbb{R}$ denote the *relevance potential* over a *d*‑dimensional latent event space.  For any point $\mathbf{x}\in\mathbb{R}^{d}$ define:

$$
\begin{aligned}
H_{\varepsilon}(\mathbf{x}) &= -\!\sum_{i=1}^{k} p_i\,\ln p_i 
&&\text{(local entropy)},\\[4pt]
\lVert\nabla\mathcal{F}\rVert &= \sqrt{\sum_{i} (\partial_i\mathcal{F})^{2}}
&&\text{(gradient magnitude)},\\[4pt]
\kappa(\mathbf{x}) &= \nabla\!\cdot\!\Bigl(
\tfrac{\nabla\mathcal{F}}{\lVert\nabla\mathcal{F}\rVert}\Bigr)
&&\text{(mean curvature)},\\[6pt]
G(\mathbf{x}) &= 
\dfrac{\lVert\Delta\mathcal{F}\rVert}{\lVert\Delta\mathbf{x}\rVert}
&&\text{(perturbation gain)}.
\end{aligned}
$$
### 9.2 Requisite Variety Metric

Ashby’s criterion compares environmental and internal dimensionalities:

$$
\begin{aligned}
V_E &= \dim\bigl(\operatorname{Span}\nabla\mathcal{F}\bigr),\\
V_A &= \operatorname{rank}(\Sigma_A),\\
\text{Variety Mismatch (VMM)} &= V_E - V_A \;\ge 0,
\end{aligned}
$$

where $\Sigma_A$ is the agent’s representational covariance matrix.

### 9.3 Compressibility‑Stress Metric (CSM)

$$
\text{CSM}(\mathbf{x}) \;=\;
w_{1}\,H_{\varepsilon} \;+\;
w_{2}\,\lVert\nabla\mathcal{F}\rVert \;+\;
w_{3}\,|\kappa| \;+\;
w_{4}\,G,
$$

with non‑negative weights \(w_{i}\) chosen per domain.

### 9.4 Critical Threshold & Ultra‑Stability

$$
\text{CTM}(\mathbf{x}) \;=\;
\alpha\,\text{CSM}(\mathbf{x}) \;+\;
\beta\,\text{VMM}.
$$

If \(\text{CTM}(\mathbf{x}) > \theta\) (an agent‑specific hyper‑parameter)  
the agent triggers an ultra‑stable reconfiguration: expanding its model‐space or shifting its Markov blanket to restore viability.

### 9.5 Reference Implementation (Koka)


```koka
module agent
open console

//  Configuration 
fun weights() = (0.4, 0.3, 0.2, 0.1)   // w1 .. w4
val alpha : double = 0.6
val beta  : double = 0.4
val theta : double = 1.0               // CTM threshold

//  Agent record -
struct Agent
  { mutable modelDim : int }

fun Agent.new(dim : int) : total Agent
{ Agent(dim) }

//  Variety & Metrics -
fun variety(a : Agent) : total int
{ a.modelDim }

fun csm(entropy : double, grad : double, curv : double, gain : double) : total double
{ val (w1,w2,w3,w4) = weights()
  w1*entropy + w2*grad + w3*abs(curv) + w4*gain }

fun ctm(csmVal : double, vmm : double) : total double
{ alpha*csmVal + beta*vmm }

//  Adaptation loop -
fun step(a : Agent,
         env : (double,double,double,double,int)) : total ()
{
  val (entropy,grad,curv,gain,VE) = env
  val csmVal = csm(entropy,grad,curv,gain)
  val vmm    = VE - variety(a)
  if (vmm < 0) pruneModel(a,abs(vmm))
  val ctmVal = ctm(csmVal, max(vmm,0))
  if (ctmVal > theta) ultraStableReconfig(a,ctmVal)
}

fun pruneModel(a : Agent, excess : int) : total ()
{ a.modelDim := max(1, a.modelDim - excess) }

fun ultraStableReconfig(a : Agent, pressure : double) : total ()
{
  a.modelDim := a.modelDim + int(pressure)
  // TODO: deform concern tensor / Markov blanket here
}
```
## Appendix A: A Formal Supplement to the Minimal Theory of Agency

_Section 9 concluded the main architecture with a set of operational metrics—translating semantic strain into adaptive thresholds. With this, the theory of agency becomes measurable, responsive, and implementable. Yet the constructs introduced throughout—semantic fields, concern tensors, fibered goal structures, and closure events—require a formal mathematical foundation to be fully specified, composed, and scaled._

_Appendix A provides this foundation. It gathers the theoretical primitives introduced across the text and presents them as formal structures within a non-metric, event-based manifold. Here we define the relevance tensor, teleological fiber bundles, contradiction functions, closure operators, and Kolmogorov thresholds as first-class mathematical objects. The aim is not to elaborate a single full model, but to make explicit the minimal assumptions required to simulate, reason about, and design emergent agency._

_This appendix grounds the theory in a composable and extensible formalism—positioning the model for future development in computational, categorical, or simulation-based systems._

This appendix introduces a formal scaffold for the concepts presented in the main text. It offers mathematical expressions for relevance, goal-structured teleology, and agent individuation via closure and complexity, in the context of a non-temporal, semantic-affordance space.
### 1. The Base Manifold of Events

Let $\mathcal{M}$ be a topological space of events, where events are composable and ordered but not metrically distributed in time or space. This base space represents a field of possible interactions, transitions, and phenomenological distinctions.

Each point $e \in \mathcal{M}$ is an event: a minimal transformation or perceptual unit within an agent’s structure of concern.

### 2. Relevance Tensor Field

We define a relevance tensor $R$ as a differentiable field over $\mathcal{M}$ that encodes semantic, teleological, or inferential salience:

$$
R: \mathcal{M} \rightarrow \mathbb{R}^n
$$

At each event $e$, the tensor $R(e)$ encodes the local orientation of attention, affordance, or concern. The geometry of $R$ warps $\mathcal{M}$ into a field of action-guidance rather than metric distance.

Curvature in $R$ corresponds to:
- Differential concern
- Probabilistic gradients of action selection
- Projective teleological flow

### 3. Fiber Bundles and Teleological Structure

At each event $e \in \mathcal{M}$, we define a fiber $F_e$ that encodes local goal conditions:

$$
F = \bigcup_{e \in \mathcal{M}} F_e
$$
Each $F_e$ is a structured set of teleological vectors—goal paths, orderings, or satisfaction conditions. These may be seen as directed graphs or categorical objects.

- Fibers encode possible local futures  
- They are dynamically reconfigured under agent inference  
- Interaction between fibers reshapes the field of relevance $R$

### 4. Probabilistic Interaction and Contradiction

Define:
- $P(g \mid e)$: the probability of achieving goal $g$ given current event $e$
- $C(F_e, F_{e'})$: a contradiction function expressing incompatibility between fibers

Where contradiction increases (e.g., mutually exclusive goal orderings), the local geometry becomes tense. Resolution is required through pruning, goal restructuring, or closure.

### 5. Closure and the Emergence of Agency

Let a closure $\mathcal{C}$ be defined over a region $U \subset \mathcal{M}$ where:
- Contradictions are irreconcilable
- $R$ becomes non-differentiable
- No projection of existing goals resolves the local configuration

Then:

$$
\mathcal{C} = \text{closure}(U) \Rightarrow \text{agent emergence}
$$

This closure is a minimal semantic individuation—a bounded zone in which internal relevance must now be defined autonomously.

### 6. Kolmogorov Closure and Complexity Threshold

Let $S$ be a system embedded in environment $E$. The Kolmogorov complexity of $S$ given $E$ is:

$$
K(S \mid E) = \text{length of shortest program that generates } S \text{ given } E
$$

If:

$$
K(S \mid E) > \tau
$$

for some tractability threshold $\tau$, then $S$ cannot be modeled or predicted using existing environmental compression. It must be treated as causally primitive.  
This is the epistemic condition for treating $S$ as an agent.

### 7. Projection, Affordance, and Curvature

Agents project goal-structured fields over $\mathcal{M}$ via their relevance tensor. This is the semantic equivalent of a light cone—a cone of possible futures warped by concern.

Let $A$ be an agent. Its action cone is:

$$
L_A = \{ e' \in \mathcal{M} \mid R(e') \cdot v > 0 \}
$$

for some relevance vector $v$ defined by $A$'s fiber. This structure allows the agent to act *prospectively*, even without metric time.

### Closing Note

This framework remains minimal in the sense that:
- It presumes only a non-metric base space,
- Relevance as projection,
- And the capacity for internal contradiction.

From these, we derive individuation, action, and recursive emergence of agents.

This formalism is not final, but opens the space for future computational, philosophical, or simulation-based expansions.


## Appendix B: Framing Assumptions and Interpretive Structure of the Model Architecture Decision Guide

_Appendix A provided the formal scaffold beneath the theory, defining its core structures within a minimal topological framework. But formalism alone does not determine application—how these structures are interpreted, prioritized, and deployed depends on deeper representational and epistemic commitments._

_Appendix B articulates those commitments. It outlines the foundational assumptions that shape the interpretation of the model architecture—clarifying what is presumed about space, time, teleology, and the emergence of agency. This interpretive layer is essential for meaningful model selection, since it reveals the structural dependencies that influence representational choices._

This appendix acts as a hermeneutic key—positioning the formal tools within a conceptual landscape and enabling principled navigation of the modeling space introduced in the next appendix.

This guide is not a checklist of features, but a structural tool for navigating the architectural space of semantic agency models.  

It does not yield binary results (e.g., tensor vs. fiber), but rather maps the consequences of particular epistemic and representational commitments.

The following assumptions and structural dependencies underlie the framework and shape how its results should be interpreted:

### 1. Representational Ground: Events as Structured Space

The guide assumes that agents operate not over raw inputs, but within a structured semantic field:  
a topological or differentiable manifold of events.

This base space—denoted $\mathcal{M}$—is the canvas on which relevance, goal-structure, contradiction, and projection are modelled.  
Both tensor and fiber models assume $\mathcal{M}$, but extend it differently.

### 2. Teleology as Emergent Structure

Goals are not assumed as primitives. Instead, teleology emerges from pressure gradients in the concern tensor or branching paths in the fiber structure.  
The framework assumes that teleology is generated within the agent, as a response to contradiction, salience, or unresolved structure.

> Projection, not planning, is the baseline operation of agency.

### 3. Agency as Compression Failure

The emergence of agency is treated not as a given, but as the system’s response to irreducibility.  
When compression, inference, or representation breaks down, the system closes—producing an individuated agent.

This closure can be modelled structurally (in fibers) or topologically (in tensors).

This aligns with the theory’s foundational claim:  
> _Agency is what arises when modeling breaks down, and concern persists._

### 4. Structure Requires Individuation

The architecture must support the emergence of new agents, closures, or loci of coherence when existing representations fail.

This demands a modeling framework that can track:
- Contradiction zones
- Projection gradients
- Concern curvature
- The transformation of relational topology

The framework assumes that the system must support recursive individuation.

### Interpreting Results

The output of the decision guide is not a simple recommendation.  
It is a profile of constraints and affordances, identifying:
- The ontological assumptions embedded in the model  
  (e.g., is goal-structure composable?)
- The representational substrate most compatible with those assumptions  
  (tensor vs. fiber)
- The computational scaling profile implied by the model
- The locus and dynamics of semantic individuation  
  (field curvature vs. structural contradiction)

This allows researchers and designers to make informed tradeoffs, balancing:
- Computational tractability
- Semantic expressivity
- Memory architecture
- Reconfigurability under surprise

In some cases, this will point toward a purely fibered architecture.  
In others, it will favour continuous concern fields.

But in many cases, it will motivate a hybrid strategy:
- Where fibers are constructed dynamically from tensor curvature
- Or where goal logic is constrained by smooth gradients of relevance

## Appendix C: Semantic Architecture Decision Guide

_Appendix B clarified the interpretive commitments that underlie model design. But for system architects and theorists working in applied settings, a structured approach to model selection is essential—especially when computational constraints, representational needs, and semantic affordances pull in different directions._

_Appendix C offers such a guide. It provides a structured framework for selecting between tensor-based, fiber-based, or hybrid agent architectures—based on conceptual, computational, and integration-oriented criteria. This is not a fixed rubric, but a map of tradeoffs: a way to navigate the space of semantic agency models in light of the tensions and transformations outlined throughout the paper._

_With this, the document closes its recursive loop—from theory to mechanism, from structure to operation, and finally, from formalism to design._

This guide is meant to help designers determine which agent formalism best fits their domain. For example, in a system requiring nested goal logic and structural reversibility, a fiber model is preferred.

This decision guide supports the selection and evaluation of agent modeling architectures. It helps determine when to use a fiber bundle model, a tensor field model, or a hybrid structure, based on the structure of goals, demands of the environment, and computational requirements.

###  1. Conceptual Design Questions
C1 Do agents need to explicitly reason about structured goals (e.g., dependencies, preconditions, nested outcomes)?  
→ Yes → Consider fiber bundles  
→ No → Tensor fields may suffice

C2 Are agent actions best described as moving through discrete plans or continuous fields of concern?  
→ Plans → Fiber  
→ Fields → Tensor

C3 Do contradictions arise structurally (logical conflict) or topologically (semantic tension)?  
→ Structural → Fiber  
→ Topological → Tensor

C4 Does your system require narrative reasoning, compositional logic, or categorical goal relations?  
→ Yes → Favor fiber model with functors  
→ No → Pure tensor gradient may be appropriate

C5 Is teleology driven more by attention/salience or goal resolution?  
→ Salience → Tensor  
→ Goal resolution → Fiber

###  2. Computational & Scalability Questions
S1 What is the dimensionality of your semantic field ($\mathbb{R}^n$)?  
→ High → Tensor cost increases exponentially  
→ Low or sparse → Tensor feasible or fiber dominant

S2 Is the agent’s perception/action stream continuous or episodic?  
→ Continuous → Tensor  
→ Episodic → Fiber

S3 Does the agent need real-time responsiveness under dynamic conditions?  
→ Yes → Fibers scale better  
→ No → Tensor deformation possible

S4 Is backward reasoning (e.g., tracing causes, undoing plans) important?  
→ Yes → Use fiber graphs with structural reversibility  
→ No → Tensor projection may be sufficient

S5 How is memory represented?  
→ Explicit narrative or event trace → Fiber  
→ Implicit semantic deformation → Tensor

###  3. Implementation & Integration Questions
I1 Is your environment symbolic, discrete, or structured by predicates?  
→ Yes → Prefer fiber-based models  
→ No → Tensor semantics may be more aligned

I2 Do you need interpretability (debuggable plans and decision logic)?  
→ Yes → Fiber bundles are clearer  
→ No → Tensor models enable emergent, fluid behaviour

I3 Does your agent operate through an inference loop or reactive modulation?  
→ Infer → Act → Rebuild → Fiber  
→ Scan → Warp → Flow → Tensor

I4 Are you integrating with classical planners or symbolic logic systems?  
→ Yes → Favour fiber with category structure  
→ No → Tensor gradients can generalise

###  4. Hybrid Viability Questions

H1 Could local fibers be generated dynamically from field curvature?  
→ Yes → Use a hybrid architecture: $F_e$ sampled from $R$

H2 Could contradiction or surprise induce fiber formation or closure?  
→ Yes → Let tensor gradients govern structural emergence

H3 Can memory and structure be stored as fiber graphs while adaptation is governed by continuous fields?  
→ Yes → Combine: tensor = modulation, fiber = logic/memory

H4 Does your computational budget allow for layered representations (field + discrete structure)?  
→ Yes → Proceed with hybrid implementation

###  Summary Use

Use this guide to profile:
- The modeling commitments embedded in your agent design
- The tradeoffs between semantic expressivity and computational feasibility
- The structural affordances of tensor, fiber, or hybrid models

→ _For framing assumptions and interpretation of results, see Appendix B._  
→ _For formal definitions of tensors, fibers, and closure mechanisms, see Appendix A._

